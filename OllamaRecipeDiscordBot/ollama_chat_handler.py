# ollama_chat_handler.py
import logging
logging.basicConfig(level=logging.INFO)

import os
from dotenv import load_dotenv

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

# Define your template and model
template = (
    "Answer the question below "
    "Here is the conversation history {context}."
    "Question: {question}."
    "Answer:"
)

# Initialize the model and prompt template
load_dotenv()
model_name = os.getenv("LLAMA_MODEL_NAME")
model = OllamaLLM(model=model_name)
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model


# Function to handle conversation
def handle_conversation(context, user_input):
    """
    Handle the conversation by invoking the chain with the given context and user input.

    Parameters:
    context (str): The conversation history.
    user_input (str): The latest input from the user.

    Returns:
    str: The response generated by the model.
    """
    try:
        result = chain.invoke({"context": context, "question": user_input})
        logging.info(f"Handled conversation with context: {context}, user_input: {user_input}")
        return result
    except Exception as e:
        logging.error(f"An error occurred: {e}")

